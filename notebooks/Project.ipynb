{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4668a816-21f1-4253-b3eb-2d0b16cee877",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimization for Machine Learning - Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a401d06b-9aea-4898-aa47-cbffd3118c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85926fc1-fb59-498a-8449-660f0a868ef9",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "(a) Generate a $300 \\times 20$ data matrix $X$, where each entry is uniformly random. Generate an outcome vector $y$, which is a linear combination of the columns of $X$ with uniformly random weights, and some Gaussian noise added to each entry of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c644e6b7-7ae7-4e2f-9a62-b955054e7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Xij ~ Unif([0, 1]), X.shape = (300, 20)\n",
    "\n",
    "# shape\n",
    "m = 300\n",
    "n = 20\n",
    "\n",
    "X = np.random.uniform(size = (m, n))\n",
    "\n",
    "# generate random noise eps, eps.shape = (300,)\n",
    "eps = np.random.normal(size = m)\n",
    "\n",
    "# uniformly random weights weight, weight.shape = (20,)\n",
    "weight = np.random.uniform(size = n)\n",
    "\n",
    "# y = X weight + eps, y.shape = (300,)\n",
    "y = X @ weight + eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261f9a1-dae5-4659-8822-06a9dfb431cc",
   "metadata": {},
   "source": [
    "(b) Write a function to divide the data set into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a266dbef-d000-4010-b25f-ea17a180ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for splitting into training and testing datasets\n",
    "def train_test_split(X, y, p: float):\n",
    "    '''\n",
    "    (X, y): datapoints\n",
    "    p: fraction of data that is in training set\n",
    "\n",
    "    currently not random split --> implement later?\n",
    "\n",
    "    returns: X_train, X_test, y_train, y_test\n",
    "    '''\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    split = int(np.ceil(p * m))\n",
    "\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "74a176c6-58cc-452e-9a35-0aca10a0e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "p = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941921f-5410-432e-b730-7eff48027a3e",
   "metadata": {},
   "source": [
    "(c) Write functions for OLS and Ridge regression and apply this to your synthetic data set. Discuss the performance on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "526d5d95-7579-4243-a181-03364133e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(X, y, w):\n",
    "    MSE = np.mean((X @ w - y)**2)\n",
    "    return MSE\n",
    "\n",
    "# unsure if need to use other method to solve equation\n",
    "def OLS_cvxpy(X_train, y_train):\n",
    "    n = X_train.shape[1]\n",
    "    \n",
    "    w = cp.Variable(n)\n",
    "    obj = cp.Minimize(cp.sum((X_train @ w - y_train)**2))\n",
    "\n",
    "    prob = cp.Problem(obj)\n",
    "    prob.solve()\n",
    "\n",
    "    return w.value\n",
    "\n",
    "def ridge_cvxpy(X_train, y_train, lam):\n",
    "    n = X_train.shape[1]\n",
    "    \n",
    "    w = cp.Variable(n)\n",
    "    obj = cp.Minimize(cp.sum((X_train @ w - y_train)**2) + lam * cp.sum(w**2))\n",
    "\n",
    "    prob = cp.Problem(obj)\n",
    "    prob.solve()\n",
    "\n",
    "    return w.value\n",
    "\n",
    "# def find_opt_lam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f7684337-309f-4b3f-acfb-b00276989c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 0.8441\n",
      "OLS test MSE: 1.1471\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.8442\n",
      "ridge test MSE: 1.1451\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "w_OLS = OLS_cvxpy(X_train, y_train)\n",
    "w_ridge = ridge_cvxpy(X_train, y_train, 0.1)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = performance(X_train, y_train, w_OLS), performance(X_test, y_test, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = performance(X_train, y_train, w_ridge), performance(X_test, y_test, w_ridge)\n",
    "\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37762593-cde2-4745-a57a-a61264c677c3",
   "metadata": {},
   "source": [
    "(d) Create a data matrix with many multicolinearities by adding a large number (say, 200) columns to X that are linear combinations of the original 20 columns with some Gaussian noise added to each entry. Run OLS and Ridge regression and discuss the performance on train and test sets. Is it hard to find a good value for $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4caf3c3c-2c05-4d71-b0ad-f3353e00eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_multicolinearity(X, num):\n",
    "    n_feats = X.shape[1]\n",
    "    n_entries = X.shape[0]\n",
    "\n",
    "    new_cols = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        # uniformly random weights w, w.shape = (n_feats, )\n",
    "        w = np.random.uniform(size = n_feats)\n",
    "\n",
    "        # generate random noise eps, eps.shape = (n_entries,)\n",
    "        eps = np.random.normal(size = n_entries)\n",
    "\n",
    "        # linear combination of features + noise\n",
    "        X_col = X @ w + eps\n",
    "        new_cols.append(X_col.reshape(-1, 1))\n",
    "\n",
    "    # concatenate the X with new columns\n",
    "    X_new = np.hstack([X] + new_cols)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "648e26cb-70b8-4d32-9c39-f7a089d9335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 0.0\n",
      "OLS test MSE: 13.164\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.0023\n",
      "ridge test MSE: 11.6859\n"
     ]
    }
   ],
   "source": [
    "X_new = add_multicolinearity(X, 200)\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, p)\n",
    "\n",
    "# example usage\n",
    "w_OLS = OLS_cvxpy(X_train_new, y_train_new)\n",
    "w_ridge = ridge_cvxpy(X_train_new, y_train_new, 0.1)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = performance(X_train_new, y_train_new, w_OLS), performance(X_test_new, y_test_new, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = performance(X_train_new, y_train_new, w_ridge), performance(X_test_new, y_test_new, w_ridge)\n",
    "\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")\n",
    "\n",
    "# still need to find optimal lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b439ffc-fff8-472f-a0a9-c92f6c095e11",
   "metadata": {},
   "source": [
    "(e) Now instead of adding multicolinearities, add many superficial feature columns to X which have no relation to the outcome vector y. Again run OLS and Ridge regression and discuss the performance on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "61d6325a-a5c4-48e0-902f-366ec6bffad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_superficial(X, num):\n",
    "    n_entries = X.shape[0]\n",
    "\n",
    "    sup_feats = np.random.uniform(size = (n_entries, num))\n",
    "    print(sup_feats.shape)\n",
    "\n",
    "    # concatenate the X with new columns\n",
    "    X_new = np.hstack([X, sup_feats])\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8f82666f-fc34-45d7-90ef-d3b8bd89458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 200)\n",
      "OLS train MSE: 0.0\n",
      "OLS test MSE: 21.2017\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.0228\n",
      "ridge test MSE: 6.3495\n"
     ]
    }
   ],
   "source": [
    "X_new = add_superficial(X, 200)\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y, p)\n",
    "\n",
    "# example usage\n",
    "w_OLS = OLS_cvxpy(X_train_new, y_train_new)\n",
    "w_ridge = ridge_cvxpy(X_train_new, y_train_new, 0.1)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = performance(X_train_new, y_train_new, w_OLS), performance(X_test_new, y_test_new, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = performance(X_train_new, y_train_new, w_ridge), performance(X_test_new, y_test_new, w_ridge)\n",
    "\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")\n",
    "\n",
    "# still need to find optimal lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358f6da-dfea-46ec-aadf-1590abadb4d8",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "(a) Implement functions for logistic regression and hinge-loss classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a094b7b2-d582-4cec-8ec9-12fc48eb6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write func for M for classification\n",
    "\n",
    "# y should be in {-1, 1} so need new y\n",
    "\n",
    "def logistic_regression(X, y, lam):\n",
    "    n, d = X.shape\n",
    "    w = cp.Variable(d)\n",
    "\n",
    "    # logistic loss: sum(log(1 + exp(-y_i * (X_i w))))\n",
    "    loss = cp.sum(cp.logistic(-cp.multiply(y, X @ w)))\n",
    "    reg = (lam / 2) * cp.sum_squares(w)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(loss + reg))\n",
    "    prob.solve()\n",
    "\n",
    "    return w.value\n",
    "\n",
    "def hinge_loss(X, y, lam):\n",
    "    n, d = X.shape\n",
    "    w = cp.Variable(d)\n",
    "\n",
    "    # hinge loss: sum(max(0, 1 - y_i * (X_i w)))\n",
    "    loss = cp.sum(cp.pos(1 - cp.multiply(y, X @ w)))\n",
    "    reg = (lam / 2) * cp.sum_squares(w)\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(loss + reg))\n",
    "    prob.solve()\n",
    "\n",
    "    return w.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d5fcb864-7d97-4bff-8f4a-89f1ecabd106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05009248, 0.0448073 , 0.05339462, 0.02219443, 0.05266317,\n",
       "       0.03364129, 0.03038353, 0.03868172, 0.01609537, 0.02674105,\n",
       "       0.06448653, 0.05117883, 0.03610319, 0.07760338, 0.0542708 ,\n",
       "       0.03220692, 0.05173475, 0.05957131, 0.02107843, 0.0306043 ])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_test = hinge_loss(X_train, y_train, 0.7)\n",
    "w_log = logistic_regression(X_train, y_train, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280739c-a503-4770-8bac-b7a8d4ada41c",
   "metadata": {},
   "source": [
    "(b) Create a random data matrix $X$ and construct an output vector $y$ by generating and a random weight vector $w$ and setting $y_i = \\text{sign}(x^T_i w)$, where $x^T_i$ is the $i$-th row of $X$. Use a test/train split and check the performance of OLS, Ridge regression, logistic regression and hinge-loss classification for binary classification. Do you see a large difference in performance between these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac471b7-5f98-41e1-95c9-184195ce29c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6ae34c2-f82f-4e22-a0d8-08e14ed9d4a5",
   "metadata": {},
   "source": [
    "(c) Now create a data set $(X, y)$ for binary classification (with $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\{âˆ’1, 1\\}^n$) such that, given a test/train split, OLS and Ridge perform very badly but logistic regression and hinge-loss classification perform well. What kind of properties of your data set are responsible for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7f2c5-ee48-444c-a331-cfd4857b3982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
