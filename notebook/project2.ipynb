{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f4da8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import JAX to use\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, random\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a1fc",
   "metadata": {},
   "source": [
    "1. Implement a dense feedforward neural network from scratch.\n",
    "\n",
    "   The implementation must be flexible with respect to:\n",
    "   - The input and output dimensions.\n",
    "   - The number of hidden layers.\n",
    "   - The number of neurons per hidden layer.\n",
    "   - The activation functions used.\n",
    "\n",
    "   This implementation will be used for the following two questions.\n",
    "\n",
    "   Choose a suitable initialization for the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e84432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a function that initializes the network parameters.\n",
    "def init_net_params(layer_widths, key):\n",
    "    \"\"\"\n",
    "    Initialize the network parameters.\n",
    "    layer_widths: list of integers, the number of neurons in each layer\n",
    "    key: jax.random.PRNGKey, the random key\n",
    "    returns: list of jax.numpy.ndarray, the network parameters\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_widths) - 1)\n",
    "\n",
    "    for i, (n_in, n_out) in enumerate(zip(layer_widths[:-1], layer_widths[1:])):\n",
    "        w_key = keys[i]\n",
    "        w = random.normal(w_key, shape=(n_in, n_out))\n",
    "        b = jnp.zeros((n_out,))\n",
    "        params.append({'w': w, 'b': b})\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27ef9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a forward pass function that computes the output of the network for a given input.\n",
    "def forward(params, x, activation):\n",
    "    \"\"\"\n",
    "    Forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of activation functions\n",
    "    activations = {\n",
    "        'relu': jax.nn.relu,\n",
    "        'sigmoid': jax.nn.sigmoid,\n",
    "        'tanh': jax.nn.tanh,\n",
    "        'softmax': jax.nn.softmax\n",
    "    }\n",
    "    activation = activations[activation]\n",
    "\n",
    "    for layer in params[:-1]:\n",
    "        x = x @ layer['w'] + layer['b']\n",
    "        x = activation(x)\n",
    "\n",
    "    # output layer\n",
    "    final_layer = params[-1]\n",
    "    return jnp.dot(x, final_layer['w']) + final_layer['b']\n",
    "\n",
    "def get_batches(x, y, batch_size=256):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples (x_batch, y_batch), each of size batch_size\n",
    "    (last batch may be smaller).\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # Use provided key or create a new one\n",
    "    key = random.PRNGKey(0)\n",
    "    perm = jax.random.permutation(key, n)\n",
    "    x_shuffled = x[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        x_batch = x_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        batches.append((x_batch, y_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_splits(x, y, train=0.8):\n",
    "    \"\"\"\n",
    "    This return a jax array of the training, validation, and test splits\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x = jnp.array(x)\n",
    "    y = jnp.array(y)\n",
    "    # Calculate split indices (as integers)\n",
    "    train_end = int(train * n)\n",
    "    test_end = train_end + int((1-train)) * n\n",
    "    \n",
    "    # Split the data\n",
    "    x_train = x[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    x_test = x[test_end:]\n",
    "    y_test = y[test_end:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def get_kfolds(x, y, k=5):\n",
    "    \"\"\"\n",
    "    Generate k-fold cross-validation splits.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "\n",
    "    fold_size = n // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Validation fold indices\n",
    "        val_start = i * fold_size\n",
    "        val_end = (i + 1) * fold_size if i < k - 1 else n\n",
    "        \n",
    "        # Validation set\n",
    "        x_val = x[val_start:val_end]\n",
    "        y_val = y[val_start:val_end]\n",
    "        \n",
    "        # Training set (everything except validation fold)\n",
    "        x_train = jnp.concatenate([x[:val_start], x[val_end:]], axis=0)\n",
    "        y_train = jnp.concatenate([y[:val_start], y[val_end:]], axis=0)\n",
    "        \n",
    "        folds.append((x_train, y_train, x_val, y_val))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c460bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the MSE loss function, we can have other loss functions\n",
    "def mse_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    MSE loss function for the network.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    preds = batched_forward(params, x, activation)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "def class_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    Classification cross-entroy loss function\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "    log_probs = jax.nn.softmax(logits, axis=1)\n",
    "    \n",
    "    nll = -log_probs[jnp.arange(y.shape[0]), y]\n",
    "    loss = jnp.mean(nll)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(params, x, y, activation, classification):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset and return accuracy and loss.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "\n",
    "    if classification:\n",
    "        preds = jnp.argmax(logits, axis=1)\n",
    "        \n",
    "        accuracy = jnp.mean(preds == y)\n",
    "        loss = class_loss(params, x, y, activation)\n",
    "        \n",
    "        return accuracy, loss\n",
    "    else:\n",
    "        return jnp.mean((logits - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3f55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now define an update function that updates the network parameters.\n",
    "def update(params, x, y, activation, lr, classification):\n",
    "    \"\"\"\n",
    "    Update function for the network parameters (basic gradient descent).\n",
    "    \"\"\"\n",
    "    loss_fn = class_loss if classification else mse_loss\n",
    "    grads = grad(loss_fn)(params, x, y, activation)\n",
    "    new_params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params\n",
    "\n",
    "# After training, save the parameters\n",
    "def save_params(params, filename='assets/params.pkl'):\n",
    "    \"\"\"Save model parameters.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "    print(f\"Parameters saved to {filename}\")\n",
    "\n",
    "def load_params(filename='assets/params.pkl'):\n",
    "    \"\"\"Load model parameters.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    print(f\"Parameters loaded from {filename}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd746a",
   "metadata": {},
   "source": [
    "2. Consider a standard benchmark dataset for classification: train a neural network to classify handwritten digits into the ten classes 0, 1,..., 9. As input for your model, use flattened vector representations of the MNIST images^1\n",
    "\n",
    "   (a) For this multiclass classification task, train your neural network with cross-entropy loss and mini-batch gradient descent. Vary the neural network architecture (layers, neurons per layer, activation functions) and training hyperparameters (learning rate, batch size, epochs). Use grid search with k-fold cross-validation (e.g., k = 5) to select promising hyperparameters. Report the accuracy and learning curves for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a855b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 70000 images\n",
      "Each image has 784 pixels (features)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "print(f\"We have {mnist.data.shape[0]} images\")\n",
    "print(f\"Each image has {mnist.data.shape[1]} pixels (features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5dc4e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_kfold(x_train, y_train, layer_widths, activation, lr, batch_size, epochs, k=5):\n",
    "    \"\"\"\n",
    "    Train the model using kfold cross-validation.\n",
    "    \"\"\"\n",
    "    folds = get_kfolds(x_train, y_train, k=k)\n",
    "    fold_results =[]\n",
    "    learning_curves = []\n",
    "\n",
    "    for fold_idx, (x_train_fold, y_train_fold, x_val_fold, y_val_fold) in enumerate(folds):\n",
    "        key = random.PRNGKey(42 + fold_idx) # we use a different key for each fold\n",
    "        params = init_net_params(layer_widths, key)\n",
    "\n",
    "        fold_history = {\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': []\n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batches = get_batches(x_train_fold, y_train_fold, batch_size=batch_size)\n",
    "\n",
    "            for x_batch, y_batch in batches:\n",
    "                params = update(params, x_batch, y_batch, activation, lr, classification=True)\n",
    "            \n",
    "            train_acc, train_loss = evaluate_model(params, x_train_fold, y_train_fold, activation, classification=True)\n",
    "            val_acc, val_loss = evaluate_model(params, x_val_fold, y_val_fold, activation, classification=True)\n",
    "\n",
    "            fold_history['train_acc'].append(float(train_acc))\n",
    "            fold_history['val_acc'].append(float(val_acc))\n",
    "            fold_history['train_loss'].append(float(train_loss))\n",
    "            fold_history['val_loss'].append(float(val_loss))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"  Fold {fold_idx+1}, Epoch {epoch}: Val Acc = {val_acc:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        final_train_acc, final_train_loss = evaluate_model(params, x_train_fold, y_train_fold, activation, classification=True)\n",
    "        final_val_acc, final_val_loss = evaluate_model(params, x_val_fold, y_val_fold, activation, classification=True)\n",
    "\n",
    "        fold_results.append((final_train_acc, final_val_acc, final_train_loss, final_val_loss))\n",
    "        learning_curves.append(fold_history)\n",
    "    \n",
    "    mean_val_acc = jnp.mean([fold[1] for fold in fold_results])\n",
    "    mean_val_loss = jnp.mean([fold[3] for fold in fold_results])\n",
    "    \n",
    "    return mean_val_acc, mean_val_loss, fold_results, learning_curves\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test, best_config):\n",
    "    \"\"\"\n",
    "    Train the best model on the full training set and evaluate on test set.\n",
    "    \"\"\"\n",
    "    layer_widths = best_config['layer_widths']\n",
    "    activation = best_config['activation']\n",
    "    lr = best_config['learning_rate']\n",
    "    batch_size = best_config['batch_size']\n",
    "    epochs = best_config['epochs']\n",
    "    \n",
    "    print(f\"\\nTraining best model on full training set...\")\n",
    "    print(f\"Configuration: {best_config}\")\n",
    "    \n",
    "    # Initialize parameters\n",
    "    key = random.PRNGKey(42)\n",
    "    params = init_net_params(layer_widths, key)\n",
    "    \n",
    "    learning_curve = {\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'train_loss': [],\n",
    "        'test_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        batches = get_batches(x_train, y_train, batch_size=batch_size)\n",
    "        \n",
    "        for x_batch, y_batch in batches:\n",
    "            params = update(params, x_batch, y_batch, activation, lr, classification=True)\n",
    "        \n",
    "        # Evaluate periodically\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            train_acc, train_loss = evaluate_model(params, x_train, y_train, activation, classification=True)\n",
    "            test_acc, test_loss = evaluate_model(params, x_test, y_test, activation, classification=True)\n",
    "            \n",
    "            learning_curve['train_acc'].append(float(train_acc))\n",
    "            learning_curve['test_acc'].append(float(test_acc))\n",
    "            learning_curve['train_loss'].append(float(train_loss))\n",
    "            learning_curve['test_loss'].append(float(test_loss))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    test_acc, test_loss = evaluate_model(params, x_test, y_test, activation, classification=True)\n",
    "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return params, test_acc, test_loss, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4a69033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search configurations\n",
    "grid_search_configs = {\n",
    "    'architectures': [\n",
    "        # 2 hidden layers\n",
    "        [784, 128, 128, 10],\n",
    "        [784, 256, 256, 10],\n",
    "        [784, 512, 512, 10],\n",
    "        \n",
    "        # 3 hidden layers\n",
    "        [784, 128, 128, 128, 10],\n",
    "        [784, 256, 256, 256, 10],\n",
    "    ],\n",
    "    \n",
    "    # Activation functions\n",
    "    'activations': ['relu', 'tanh', 'sigmoid'],\n",
    "    \n",
    "    # Learning rates\n",
    "    'learning_rates': [0.001, 0.01, 0.1],\n",
    "    \n",
    "    # Batch sizes\n",
    "    'batch_sizes': [128, 256, 512],\n",
    "    \n",
    "    # Epochs\n",
    "    'epochs': [200, 500]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "20bb31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the grid search function\n",
    "def grid_search(x_train, y_train, configs, k=5):\n",
    "    \"\"\"\n",
    "    Perform grid search on the space of possible parameters defined earlier using k-fold cross validation\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate all combinations\n",
    "    combinations = list(itertools.product(\n",
    "        configs['architectures'],\n",
    "        configs['activations'],\n",
    "        configs['learning_rates'],\n",
    "        configs['batch_sizes'],\n",
    "        configs['epochs']\n",
    "    ))\n",
    "\n",
    "    total_combinations = len(combinations)\n",
    "    print(f\"Total configurations to test: {total_combinations}\")\n",
    "    print(f\"With k={k} folds, total training runs: {total_combinations * k}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    best_acc = -1\n",
    "    best_loss = -1\n",
    "    best_config = None\n",
    "    best_learning_curves = None\n",
    "    best_fold_results = None\n",
    "\n",
    "    for idx, (layer_widths, activation, lr, batch_size, epochs) in enumerate(combinations):\n",
    "        print(f\"\\n[{idx+1}/{total_combinations}] Testing configuration:\")\n",
    "        print(f\"Architecture: {layer_widths}\")\n",
    "        print(f\"Activation: {activation}\")\n",
    "        print(f\"Learning Rate: {lr}\")\n",
    "        print(f\"Batch Size: {batch_size}\")\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "        \n",
    "        mean_val_acc, mean_val_loss, fold_results, learning_curves = train_model_kfold(\n",
    "            x_train, y_train, layer_widths, activation, lr, batch_size, epochs, k=k\n",
    "        )\n",
    "        \n",
    "        # result\n",
    "        config = {\n",
    "            'layer_widths': layer_widths,\n",
    "            'activation': activation,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs\n",
    "        }\n",
    "        \n",
    "        print(f\"Mean Validation Accuracy: {mean_val_acc:.4f} ({mean_val_acc*100:.2f}%)\")\n",
    "        print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "        \n",
    "        # Update best configuration\n",
    "        if mean_val_acc > best_acc:\n",
    "            best_acc = mean_val_acc\n",
    "            best_loss = mean_val_loss\n",
    "            best_config = config.copy()\n",
    "            best_learning_curves = learning_curves\n",
    "            best_fold_results = fold_results\n",
    "            print(f\"New best configuration!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Grid Search Complete!\")\n",
    "    print(f\"Best Configuration:\")\n",
    "    print(f\"Architecture: {best_config['layer_widths']}\")\n",
    "    print(f\"Activation: {best_config['activation']}\")\n",
    "    print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "    print(f\"Batch Size: {best_config['batch_size']}\")\n",
    "    print(f\"Epochs: {best_config['epochs']}\")\n",
    "    print(f\"Best Mean Validation Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return best_config, best_acc, best_loss, best_learning_curves, best_fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33f92399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations to test: 270\n",
      "With k=5 folds, total training runs: 1350\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/270] Testing configuration:\n",
      "Architecture: [784, 128, 128, 10]\n",
      "Activation: relu\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 128\n",
      "Epochs: 200\n",
      "  Fold 1, Epoch 0: Val Acc = 0.1203, Val Loss = -0.1203\n",
      "  Fold 1, Epoch 50: Val Acc = 0.1203, Val Loss = -0.1203\n",
      "  Fold 1, Epoch 100: Val Acc = 0.1203, Val Loss = -0.1203\n",
      "  Fold 1, Epoch 150: Val Acc = 0.1203, Val Loss = -0.1203\n",
      "  Fold 2, Epoch 0: Val Acc = 0.0815, Val Loss = -0.0815\n",
      "  Fold 2, Epoch 50: Val Acc = 0.0815, Val Loss = -0.0815\n",
      "  Fold 2, Epoch 100: Val Acc = 0.0815, Val Loss = -0.0815\n",
      "  Fold 2, Epoch 150: Val Acc = 0.0815, Val Loss = -0.0815\n",
      "  Fold 3, Epoch 0: Val Acc = 0.0756, Val Loss = -0.0756\n",
      "  Fold 3, Epoch 50: Val Acc = 0.0756, Val Loss = -0.0756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m x_train, y_train, x_test, y_test \u001b[38;5;241m=\u001b[39m get_splits(x, y)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run grid search to find best configs\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m best_config, best_acc, best_loss, best_learning_curves, best_fold_results \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid_search_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print out the best accuracy and learning curve for kfold training\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe best configuration is:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[74], line 35\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(x_train, y_train, configs, k)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m mean_val_acc, mean_val_loss, fold_results, learning_curves \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_kfold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_widths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# result\u001b[39;00m\n\u001b[1;32m     40\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_widths\u001b[39m\u001b[38;5;124m'\u001b[39m: layer_widths,\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m: activation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: epochs\n\u001b[1;32m     46\u001b[0m }\n",
      "Cell \u001b[0;32mIn[72], line 24\u001b[0m, in \u001b[0;36mtrain_model_kfold\u001b[0;34m(x_train, y_train, layer_widths, activation, lr, batch_size, epochs, k)\u001b[0m\n\u001b[1;32m     21\u001b[0m batches \u001b[38;5;241m=\u001b[39m get_batches(x_train_fold, y_train_fold, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[0;32m---> 24\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m train_acc, train_loss \u001b[38;5;241m=\u001b[39m evaluate_model(params, x_train_fold, y_train_fold, activation, classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m evaluate_model(params, x_val_fold, y_val_fold, activation, classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[70], line 7\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, x, y, activation, lr, classification)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mUpdate function for the network parameters (basic gradient descent).\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m class_loss \u001b[38;5;28;01mif\u001b[39;00m classification \u001b[38;5;28;01melse\u001b[39;00m mse_loss\n\u001b[0;32m----> 7\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m new_params \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m p, g: p \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m g, params, grads)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_params\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api.py:399\u001b[0m, in \u001b[0;36mgrad.<locals>.grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fun, docstr\u001b[38;5;241m=\u001b[39mdocstr, argnums\u001b[38;5;241m=\u001b[39margnums)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgrad_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 399\u001b[0m   _, g \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_and_grad_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api.py:470\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.value_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m   _check_input_dtype_grad(holomorphic, allow_int, leaf)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[0;32m--> 470\u001b[0m   ans, vjp_py \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdyn_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    472\u001b[0m   ans, vjp_py, aux \u001b[38;5;241m=\u001b[39m _vjp(\n\u001b[1;32m    473\u001b[0m       f_partial, \u001b[38;5;241m*\u001b[39mdyn_args, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api.py:2015\u001b[0m, in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[1;32m   2014\u001b[0m   flat_fun, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun_nokwargs(fun, in_tree)\n\u001b[0;32m-> 2015\u001b[0m   out_primals, vjp \u001b[38;5;241m=\u001b[39m \u001b[43mad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvjp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2016\u001b[0m   out_tree \u001b[38;5;241m=\u001b[39m out_tree()\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:260\u001b[0m, in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvjp\u001b[39m(traceable: lu\u001b[38;5;241m.\u001b[39mWrappedFun, primals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    259\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_aux:\n\u001b[0;32m--> 260\u001b[0m     out_primals, pvals, jaxpr, consts \u001b[38;5;241m=\u001b[39m \u001b[43mlinearize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     out_primals, pvals, jaxpr, consts, aux \u001b[38;5;241m=\u001b[39m linearize(traceable, \u001b[38;5;241m*\u001b[39mprimals, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:245\u001b[0m, in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m _, in_tree \u001b[38;5;241m=\u001b[39m tree_flatten(((primals, primals), {}))\n\u001b[1;32m    244\u001b[0m jvpfun_flat, out_tree \u001b[38;5;241m=\u001b[39m flatten_fun(jvpfun, in_tree)\n\u001b[0;32m--> 245\u001b[0m jaxpr, out_pvals, consts \u001b[38;5;241m=\u001b[39m \u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_to_jaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjvpfun_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m out_primals_pvals, out_tangents_pvals \u001b[38;5;241m=\u001b[39m tree_unflatten(out_tree(), out_pvals)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m out_primal_pval\u001b[38;5;241m.\u001b[39mis_known() \u001b[38;5;28;01mfor\u001b[39;00m out_primal_pval \u001b[38;5;129;01min\u001b[39;00m out_primals_pvals):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/profiler.py:334\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    333\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:576\u001b[0m, in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    574\u001b[0m fun \u001b[38;5;241m=\u001b[39m trace_to_subjaxpr_nounits(fun, trace, instantiate, fun\u001b[38;5;241m.\u001b[39mdebug_info)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mset_current_trace(trace):\n\u001b[0;32m--> 576\u001b[0m   jaxpr, (out_pvals, consts, env) \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trace, fun\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/linear_util.py:210\u001b[0m, in \u001b[0;36mWrappedFun.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the transformed function\"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_transformed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:590\u001b[0m, in \u001b[0;36mtrace_to_subjaxpr_nounits\u001b[0;34m(f, trace, instantiate, debug_info, in_pvals)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@lu\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation2\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrace_to_subjaxpr_nounits\u001b[39m(\n\u001b[1;32m    584\u001b[0m     f: Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m     debug_info: core\u001b[38;5;241m.\u001b[39mDebugInfo,\n\u001b[1;32m    588\u001b[0m     in_pvals: Sequence[PartialVal]):\n\u001b[1;32m    589\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(pv, PartialVal) \u001b[38;5;28;01mfor\u001b[39;00m pv \u001b[38;5;129;01min\u001b[39;00m in_pvals), in_pvals\n\u001b[0;32m--> 590\u001b[0m   out_tracers, jaxpr, out_consts, env \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_to_subjaxpr_nounits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m      \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstantiate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_pvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m   out_pvals \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mpval \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m out_tracers]\n\u001b[1;32m    593\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m out_tracers\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py:623\u001b[0m, in \u001b[0;36m_trace_to_subjaxpr_nounits\u001b[0;34m(f, trace, instantiate, in_pvals, debug_info)\u001b[0m\n\u001b[1;32m    621\u001b[0m in_args \u001b[38;5;241m=\u001b[39m merge_lists(in_knowns, in_tracers, in_consts)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mset_current_trace(trace):\n\u001b[0;32m--> 623\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ans, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)), (\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected return type when tracing function to jaxpr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, core\u001b[38;5;241m.\u001b[39mTracer) \u001b[38;5;129;01mor\u001b[39;00m core\u001b[38;5;241m.\u001b[39mvalid_jaxtype(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ans), (\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected return type when tracing function to jaxpr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mans\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api_util.py:73\u001b[0m, in \u001b[0;36mflatten_fun\u001b[0;34m(f, store, in_tree, *args_flat)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@lu\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_with_aux2\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflatten_fun\u001b[39m(f: Callable, store: lu\u001b[38;5;241m.\u001b[39mStore,\n\u001b[1;32m     71\u001b[0m                 in_tree: PyTreeDef, \u001b[38;5;241m*\u001b[39margs_flat):\n\u001b[1;32m     72\u001b[0m   py_args, py_kwargs \u001b[38;5;241m=\u001b[39m tree_unflatten(in_tree, args_flat)\n\u001b[0;32m---> 73\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpy_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpy_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m   ans, out_tree \u001b[38;5;241m=\u001b[39m tree_flatten(ans)\n\u001b[1;32m     75\u001b[0m   store\u001b[38;5;241m.\u001b[39mstore(out_tree)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:80\u001b[0m, in \u001b[0;36mjvpfun\u001b[0;34m(f, instantiate, transform_stack, primals, tangents)\u001b[0m\n\u001b[1;32m     77\u001b[0m ctx \u001b[38;5;241m=\u001b[39m (source_info_util\u001b[38;5;241m.\u001b[39mtransform_name_stack(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjvp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m transform_stack\n\u001b[1;32m     78\u001b[0m        \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext())\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 80\u001b[0m   out_primals, out_tangents \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtangents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(instantiate) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     82\u001b[0m   instantiate \u001b[38;5;241m=\u001b[39m [instantiate] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_tangents)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/interpreters/ad.py:120\u001b[0m, in \u001b[0;36mjvp_subtrace\u001b[0;34m(f, tag, primals, tangents)\u001b[0m\n\u001b[1;32m    117\u001b[0m   in_tracers \u001b[38;5;241m=\u001b[39m [maybe_jvp_tracer(trace, x, t)\n\u001b[1;32m    118\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(primals, tangents)]\n\u001b[1;32m    119\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m core\u001b[38;5;241m.\u001b[39mset_current_trace(trace):\n\u001b[0;32m--> 120\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_tracers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m   out \u001b[38;5;241m=\u001b[39m unzip2(\u001b[38;5;28mmap\u001b[39m(trace\u001b[38;5;241m.\u001b[39mto_primal_tangent_pair, ans))\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api_util.py:90\u001b[0m, in \u001b[0;36mflatten_fun_nokwargs\u001b[0;34m(f, store, in_tree, *args_flat)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;129m@lu\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation_with_aux2\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mflatten_fun_nokwargs\u001b[39m(f: Callable, store: lu\u001b[38;5;241m.\u001b[39mStore,\n\u001b[1;32m     88\u001b[0m                          in_tree: PyTreeDef, \u001b[38;5;241m*\u001b[39margs_flat):\n\u001b[1;32m     89\u001b[0m   py_args \u001b[38;5;241m=\u001b[39m tree_unflatten(in_tree, args_flat)\n\u001b[0;32m---> 90\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpy_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m   ans, out_tree \u001b[38;5;241m=\u001b[39m tree_flatten(ans)\n\u001b[1;32m     92\u001b[0m   store\u001b[38;5;241m.\u001b[39mstore(out_tree)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/api_util.py:284\u001b[0m, in \u001b[0;36m_argnums_partial\u001b[0;34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mnext\u001b[39m(fixed_args_)\u001b[38;5;241m.\u001b[39mval \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m sentinel \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(fixed_args_, sentinel) \u001b[38;5;129;01mis\u001b[39;00m sentinel\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/linear_util.py:370\u001b[0m, in \u001b[0;36m_get_result_paths_thunk\u001b[0;34m(_fun, _store, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;129m@transformation_with_aux2\u001b[39m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_result_paths_thunk\u001b[39m(_fun: Callable, _store: Store, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 370\u001b[0m   ans \u001b[38;5;241m=\u001b[39m \u001b[43m_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m   result_paths \u001b[38;5;241m=\u001b[39m [_clean_keystr_arg_names(path) \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m generate_key_paths(ans)]\n\u001b[1;32m    372\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _store:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# In some instances a lu.WrappedFun is called multiple times, e.g.,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# the bwd function in a custom_vjp\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 18\u001b[0m, in \u001b[0;36mclass_loss\u001b[0;34m(params, x, y, activation)\u001b[0m\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m batched_forward(params, x, activation)\n\u001b[1;32m     16\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m nll \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(nll)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1060\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1060\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:652\u001b[0m, in \u001b[0;36m_getitem\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_getitem\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[0;32m--> 652\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:628\u001b[0m, in \u001b[0;36mrewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value, out_sharding)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    627\u001b[0m treedef, static_idx, dynamic_idx \u001b[38;5;241m=\u001b[39m split_index_for_jit(idx, arr\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreedef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_are_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m               \u001b[49m\u001b[43munique_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:637\u001b[0m, in \u001b[0;36m_gather\u001b[0;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value, out_sharding)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m    635\u001b[0m             unique_indices, mode, fill_value, out_sharding):\n\u001b[1;32m    636\u001b[0m   idx \u001b[38;5;241m=\u001b[39m merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[0;32m--> 637\u001b[0m   indexer \u001b[38;5;241m=\u001b[39m \u001b[43mindex_to_gather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shared with _scatter_update\u001b[39;00m\n\u001b[1;32m    638\u001b[0m   y \u001b[38;5;241m=\u001b[39m arr\n\u001b[1;32m    640\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:792\u001b[0m, in \u001b[0;36mindex_to_gather\u001b[0;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[1;32m    789\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m normalize_indices:\n\u001b[1;32m    790\u001b[0m     advanced_pairs \u001b[38;5;241m=\u001b[39m ((_normalize_index(e, x_shape[j]), i, j)\n\u001b[1;32m    791\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m e, i, j \u001b[38;5;129;01min\u001b[39;00m advanced_pairs)\n\u001b[0;32m--> 792\u001b[0m   advanced_indexes, idx_advanced_axes, x_advanced_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madvanced_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Current axis in x.\u001b[39;00m\n\u001b[1;32m    795\u001b[0m y_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Current axis in y, before collapsing. See below.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:790\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    786\u001b[0m   advanced_pairs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    787\u001b[0m     (lax_numpy\u001b[38;5;241m.\u001b[39masarray(e), i, j) \u001b[38;5;28;01mfor\u001b[39;00m j, (i, e) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx_no_nones)\n\u001b[1;32m    788\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lax_numpy\u001b[38;5;241m.\u001b[39misscalar(e) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (Sequence, Array, np\u001b[38;5;241m.\u001b[39mndarray)))\n\u001b[1;32m    789\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m normalize_indices:\n\u001b[0;32m--> 790\u001b[0m     advanced_pairs \u001b[38;5;241m=\u001b[39m ((\u001b[43m_normalize_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, i, j)\n\u001b[1;32m    791\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m e, i, j \u001b[38;5;129;01min\u001b[39;00m advanced_pairs)\n\u001b[1;32m    792\u001b[0m   advanced_indexes, idx_advanced_axes, x_advanced_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39madvanced_pairs)\n\u001b[1;32m    794\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Current axis in x.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/indexing.py:201\u001b[0m, in \u001b[0;36m_normalize_index\u001b[0;34m(index, axis_size)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39madd(index, axis_size_val) \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m index\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mselect(\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m, lax\u001b[38;5;241m.\u001b[39madd(index, axis_size_val), index)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:579\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 579\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/jax_env/lib/python3.10/site-packages/jax/_src/traceback_util.py:176\u001b[0m, in \u001b[0;36mapi_boundary.<locals>.reraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapi_boundary\u001b[39m(fun: C) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m C:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m'''Wraps ``fun`` to form a boundary for filtering exception tracebacks.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m  When an exception occurs below ``fun``, this appends to it a custom\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m  traceback that excludes the frames specific to JAX's implementation.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m  '''\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m   \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fun)\n\u001b[1;32m    177\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreraise_with_filtered_traceback\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    178\u001b[0m     __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run this if you want to search and train, but hopefully some params are saved :)\n",
    "x = mnist.data.to_numpy()\n",
    "y = mnist.target.astype(int).to_numpy()\n",
    "x_train, y_train, x_test, y_test = get_splits(x, y)\n",
    "\n",
    "# Run grid search to find best configs\n",
    "best_config, best_acc, best_loss, best_learning_curves, best_fold_results = grid_search(\n",
    "    x_train, y_train, \n",
    "    grid_search_configs, \n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Print out the best accuracy and learning curve for kfold training\n",
    "print(\"The best configuration is:\")\n",
    "print(best_config)\n",
    "print(f\"Best K-fold Mean Accuracy: {best_acc:.2f} ({best_acc*100:.2f}%)\")\n",
    "print(f\"Best K-fold Mean Loss: {best_loss:.2f}\")\n",
    "\n",
    "# Train best model on these params\n",
    "params, test_acc, test_loss, learning_curve = train_model(\n",
    "    x_train, y_train, x_test, y_test, best_config\n",
    ")\n",
    "\n",
    "# Print out the best accuracy and learning curve\n",
    "print(f\"Our best model trained on the full training set has an accuracy of {test_acc}%\")\n",
    "print(f\"Our best model trained on the full training set has a loss of {test_loss:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(learning_curve['train_acc'], label='Training Accuracy')\n",
    "plt.plot(learning_curve['test_acc'], label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(learning_curve['train_loss'], label='Training Loss')\n",
    "plt.plot(learning_curve['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "save_params(params, filename='assets/mnist_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ead2d0",
   "metadata": {},
   "source": [
    "   (b) Study how optimizer hyperparameters (batch size, learning rate) affect convergence speed and final performance, and discuss your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94dd50e",
   "metadata": {},
   "source": [
    " (c) Identify and visualize misclassified images for your best model, and provide possible explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
