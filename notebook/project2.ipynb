{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4da8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import JAX to use for differentiation\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55c14307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a dense feedforward neural network from scratch. Make your implementation flexible with\n",
    "# respect to the input and output dimensions, the number of hidden layers and neurons per hidden\n",
    "# layer, and the activation functions used. Use this implementation for the following two questions.\n",
    "# Choose a suitable initialization for the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e84432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a function that initializes the network parameters.\n",
    "def init_net_params(layer_widths, key):\n",
    "    \"\"\"\n",
    "    Initialize the network parameters.\n",
    "    layer_widths: list of integers, the number of neurons in each layer\n",
    "    key: jax.random.PRNGKey, the random key\n",
    "    returns: list of jax.numpy.ndarray, the network parameters\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_widths) - 1)\n",
    "\n",
    "    for i, (n_in, n_out) in enumerate(zip(layer_widths[:-1], layer_widths[1:])):\n",
    "        w_key = keys[i]\n",
    "        w = random.normal(w_key, shape=(n_in, n_out))\n",
    "        b = jnp.zeros((n_out,))\n",
    "        params.append({'w': w, 'b': b})\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27ef9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a forward pass function that computes the output of the network for a given input.\n",
    "def forward(params, x, activation):\n",
    "    \"\"\"\n",
    "    Forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of activation functions\n",
    "    activations = {\n",
    "        'relu': jax.nn.relu,\n",
    "        'sigmoid': jax.nn.sigmoid,\n",
    "        'tanh': jax.nn.tanh,\n",
    "        'softmax': jax.nn.softmax\n",
    "    }\n",
    "    activation = activations[activation]\n",
    "\n",
    "    for layer in params[:-1]:\n",
    "        x = x @ layer['w'] + layer['b']\n",
    "        x = activation(x)\n",
    "\n",
    "    # output layer\n",
    "    final_layer = params[-1]\n",
    "    return jnp.dot(x, final_layer['w']) + final_layer['b']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c460bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the loss function.\n",
    "def loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    Loss function for the network.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    preds = batched_forward(params, x, activation)\n",
    "    return jnp.mean((preds - y) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d3f55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now define an update function that updates the network parameters.\n",
    "def update(params, x, y, activation, lr):\n",
    "    \"\"\"\n",
    "    Update function for the network parameters.\n",
    "    \"\"\"\n",
    "    grads = grad(loss)(params, x, y, activation)\n",
    "    new_params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9a25167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0, Loss: 0.5238784551620483\n",
      "Epoch 100, Loss: 0.250282347202301\n",
      "Epoch 200, Loss: 0.25010988116264343\n",
      "Epoch 300, Loss: 0.2500782012939453\n",
      "Epoch 400, Loss: 0.25005677342414856\n",
      "Epoch 500, Loss: 0.2500416338443756\n",
      "Epoch 600, Loss: 0.2500308156013489\n",
      "Epoch 700, Loss: 0.2500229477882385\n",
      "Epoch 800, Loss: 0.2500171661376953\n",
      "Epoch 900, Loss: 0.25001290440559387\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# we test it now\n",
    "key = random.PRNGKey(42)\n",
    "layer_widths = [2, 3, 1]\n",
    "epochs = 1000\n",
    "params = init_net_params(layer_widths, key)\n",
    "x = jnp.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = jnp.array([0.0, 1.0, 1.0, 0.0])\n",
    "activation = 'relu'\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    params = update(params, x, y, activation, lr=0.1)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss(params, x, y, activation)}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a855b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider a standard benchmark dataset for classification: train a neural network to classify handwrit-\n",
    "# ten digits into the ten classes 0, 1, . . . , 9. As input for your model, use flattened vector representations\n",
    "# of the MNIST images\n",
    "mnist = fetch_openml('mnist_784')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
