{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4da8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import JAX to use\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, random\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pickle\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223a1fc",
   "metadata": {},
   "source": [
    "1. Implement a dense feedforward neural network from scratch.\n",
    "\n",
    "   The implementation must be flexible with respect to:\n",
    "   - The input and output dimensions.\n",
    "   - The number of hidden layers.\n",
    "   - The number of neurons per hidden layer.\n",
    "   - The activation functions used.\n",
    "\n",
    "   This implementation will be used for the following two questions.\n",
    "\n",
    "   Choose a suitable initialization for the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e84432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a function that initializes the network parameters.\n",
    "def init_net_params(layer_widths, key):\n",
    "    \"\"\"\n",
    "    Initialize the network parameters.\n",
    "    layer_widths: list of integers, the number of neurons in each layer\n",
    "    key: jax.random.PRNGKey, the random key\n",
    "    returns: list of jax.numpy.ndarray, the network parameters\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_widths) - 1)\n",
    "\n",
    "    for i, (n_in, n_out) in enumerate(zip(layer_widths[:-1], layer_widths[1:])):\n",
    "        w_key = keys[i]\n",
    "        scale = jnp.sqrt(2.0 / n_in) # xavier initialization\n",
    "        w = random.normal(w_key, shape=(n_in, n_out)) * scale\n",
    "        b = jnp.zeros((n_out,))\n",
    "        params.append({'w': w, 'b': b})\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27ef9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a forward pass function that computes the output of the network for a given input.\n",
    "def forward(params, x, activation):\n",
    "    \"\"\"\n",
    "    Forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of activation functions\n",
    "    activations = {\n",
    "        'relu': jax.nn.relu,\n",
    "        'sigmoid': jax.nn.sigmoid,\n",
    "        'tanh': jax.nn.tanh,\n",
    "        'softmax': jax.nn.softmax\n",
    "    }\n",
    "    activation = activations[activation]\n",
    "\n",
    "    for layer in params[:-1]:\n",
    "        x = x @ layer['w'] + layer['b']\n",
    "        x = activation(x)\n",
    "\n",
    "    # output layer\n",
    "    final_layer = params[-1]\n",
    "    return jnp.dot(x, final_layer['w']) + final_layer['b']\n",
    "\n",
    "def get_batches(x, y, batch_size=256):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples (x_batch, y_batch), each of size batch_size\n",
    "    (last batch may be smaller).\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # Use provided key or create a new one\n",
    "    key = random.PRNGKey(0)\n",
    "    perm = jax.random.permutation(key, n)\n",
    "    x_shuffled = x[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        x_batch = x_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        batches.append((x_batch, y_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_splits(x, y, train=0.8):\n",
    "    \"\"\"\n",
    "    This return a jax array of the training, validation, and test splits\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x = jnp.array(x) / 255.0 # normalize to [0,1]\n",
    "    y = jnp.array(y)\n",
    "    # Calculate split indices (as integers)\n",
    "    train_end = int(train * n)\n",
    "    test_end = train_end + int((1-train)) * n\n",
    "    \n",
    "    # Split the data\n",
    "    x_train = x[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    x_test = x[test_end:]\n",
    "    y_test = y[test_end:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def get_kfolds(x, y, k=5):\n",
    "    \"\"\"\n",
    "    Generate k-fold cross-validation splits.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "\n",
    "    fold_size = n // k\n",
    "    folds = []\n",
    "\n",
    "    for i in range(k):\n",
    "        # Validation fold indices\n",
    "        val_start = i * fold_size\n",
    "        val_end = (i + 1) * fold_size if i < k - 1 else n\n",
    "        \n",
    "        # Validation set\n",
    "        x_val = x[val_start:val_end]\n",
    "        y_val = y[val_start:val_end]\n",
    "        \n",
    "        # Training set (everything except validation fold)\n",
    "        x_train = jnp.concatenate([x[:val_start], x[val_end:]], axis=0)\n",
    "        y_train = jnp.concatenate([y[:val_start], y[val_end:]], axis=0)\n",
    "        \n",
    "        folds.append((x_train, y_train, x_val, y_val))\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c460bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the MSE loss function, we can have other loss functions\n",
    "def mse_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    MSE loss function for the network.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    preds = batched_forward(params, x, activation)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "def class_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    Classification cross-entroy loss function\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=1)\n",
    "    \n",
    "    nll = -log_probs[jnp.arange(y.shape[0]), y]\n",
    "    loss = jnp.mean(nll)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(params, x, y, activation, classification):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset and return accuracy and loss.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "\n",
    "    if classification:\n",
    "        preds = jnp.argmax(logits, axis=1)\n",
    "        \n",
    "        accuracy = jnp.mean(preds == y)\n",
    "        loss = class_loss(params, x, y, activation)\n",
    "        \n",
    "        return accuracy, loss\n",
    "    else:\n",
    "        return jnp.mean((logits - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3f55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now define an update function that updates the network parameters.\n",
    "def update(params, x, y, activation, lr, classification):\n",
    "    \"\"\"\n",
    "    Update function for the network parameters (basic gradient descent).\n",
    "    \"\"\"\n",
    "    loss_fn = class_loss if classification else mse_loss\n",
    "    grads = grad(loss_fn)(params, x, y, activation)\n",
    "    new_params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params\n",
    "\n",
    "# After training, save the parameters\n",
    "def save_params(params, filename='assets/params.pkl'):\n",
    "    \"\"\"Save model parameters.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "    print(f\"Parameters saved to {filename}\")\n",
    "\n",
    "def load_params(filename='assets/params.pkl'):\n",
    "    \"\"\"Load model parameters.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    print(f\"Parameters loaded from {filename}\")\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd746a",
   "metadata": {},
   "source": [
    "2. Consider a standard benchmark dataset for classification: train a neural network to classify handwritten digits into the ten classes 0, 1,..., 9. As input for your model, use flattened vector representations of the MNIST images^1\n",
    "\n",
    "   (a) For this multiclass classification task, train your neural network with cross-entropy loss and mini-batch gradient descent. Vary the neural network architecture (layers, neurons per layer, activation functions) and training hyperparameters (learning rate, batch size, epochs). Use grid search with k-fold cross-validation (e.g., k = 5) to select promising hyperparameters. Report the accuracy and learning curves for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a855b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 70000 images\n",
      "Each image has 784 pixels (features)\n"
     ]
    }
   ],
   "source": [
    "mnist = fetch_openml('mnist_784')\n",
    "print(f\"We have {mnist.data.shape[0]} images\")\n",
    "print(f\"Each image has {mnist.data.shape[1]} pixels (features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dc4e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_kfold(x_train, y_train, layer_widths, activation, lr, batch_size, epochs, k=5):\n",
    "    \"\"\"\n",
    "    Train the model using kfold cross-validation.\n",
    "    \"\"\"\n",
    "    folds = get_kfolds(x_train, y_train, k=k)\n",
    "    fold_results =[]\n",
    "    learning_curves = []\n",
    "\n",
    "    for fold_idx, (x_train_fold, y_train_fold, x_val_fold, y_val_fold) in enumerate(folds):\n",
    "        key = random.PRNGKey(42 + fold_idx) # we use a different key for each fold\n",
    "        params = init_net_params(layer_widths, key)\n",
    "\n",
    "        fold_history = {\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'train_loss': [],\n",
    "            'val_loss': []\n",
    "        }\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batches = get_batches(x_train_fold, y_train_fold, batch_size=batch_size)\n",
    "\n",
    "            for x_batch, y_batch in batches:\n",
    "                params = update(params, x_batch, y_batch, activation, lr, classification=True)\n",
    "            \n",
    "            train_acc, train_loss = evaluate_model(params, x_train_fold, y_train_fold, activation, classification=True)\n",
    "            val_acc, val_loss = evaluate_model(params, x_val_fold, y_val_fold, activation, classification=True)\n",
    "\n",
    "            fold_history['train_acc'].append(float(train_acc))\n",
    "            fold_history['val_acc'].append(float(val_acc))\n",
    "            fold_history['train_loss'].append(float(train_loss))\n",
    "            fold_history['val_loss'].append(float(val_loss))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Fold {fold_idx+1}, Epoch {epoch}: Val Acc = {val_acc:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        final_train_acc, final_train_loss = evaluate_model(params, x_train_fold, y_train_fold, activation, classification=True)\n",
    "        final_val_acc, final_val_loss = evaluate_model(params, x_val_fold, y_val_fold, activation, classification=True)\n",
    "\n",
    "        fold_results.append((final_train_acc, final_val_acc, final_train_loss, final_val_loss))\n",
    "        learning_curves.append(fold_history)\n",
    "    \n",
    "    mean_val_acc = np.mean([fold[1] for fold in fold_results])\n",
    "    mean_val_loss = np.mean([fold[3] for fold in fold_results])\n",
    "    \n",
    "    return mean_val_acc, mean_val_loss, fold_results, learning_curves\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test, best_config):\n",
    "    \"\"\"\n",
    "    Train the best model on the full training set and evaluate on test set.\n",
    "    \"\"\"\n",
    "    layer_widths = best_config['layer_widths']\n",
    "    activation = best_config['activation']\n",
    "    lr = best_config['learning_rate']\n",
    "    batch_size = best_config['batch_size']\n",
    "    epochs = best_config['epochs']\n",
    "    \n",
    "    print(f\"\\nTraining best model on full training set...\")\n",
    "    print(f\"Configuration: {best_config}\")\n",
    "    \n",
    "    # Initialize parameters\n",
    "    key = random.PRNGKey(42)\n",
    "    params = init_net_params(layer_widths, key)\n",
    "    \n",
    "    learning_curve = {\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'train_loss': [],\n",
    "        'test_loss': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        batches = get_batches(x_train, y_train, batch_size=batch_size)\n",
    "        \n",
    "        for x_batch, y_batch in batches:\n",
    "            params = update(params, x_batch, y_batch, activation, lr, classification=True)\n",
    "        \n",
    "        # Evaluate periodically\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            train_acc, train_loss = evaluate_model(params, x_train, y_train, activation, classification=True)\n",
    "            test_acc, test_loss = evaluate_model(params, x_test, y_test, activation, classification=True)\n",
    "            \n",
    "            learning_curve['train_acc'].append(float(train_acc))\n",
    "            learning_curve['test_acc'].append(float(test_acc))\n",
    "            learning_curve['train_loss'].append(float(train_loss))\n",
    "            learning_curve['test_loss'].append(float(test_loss))\n",
    "            \n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Acc = {train_acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    test_acc, test_loss = evaluate_model(params, x_test, y_test, activation, classification=True)\n",
    "    print(f\"\\nFinal Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"Final Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return params, test_acc, test_loss, learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a69033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search configurations\n",
    "grid_search_configs = {\n",
    "    'architectures': [\n",
    "        # 2 hidden layers\n",
    "        [784, 128, 128, 10],\n",
    "        [784, 256, 256, 10],\n",
    "        [784, 512, 512, 10],\n",
    "        \n",
    "        # 3 hidden layers\n",
    "        [784, 128, 128, 128, 10],\n",
    "    ],\n",
    "    \n",
    "    # Activation functions\n",
    "    'activations': ['relu'],\n",
    "    \n",
    "    # Learning rates\n",
    "    'learning_rates': [0.001, 0.01, 0.1],\n",
    "    \n",
    "    # Batch sizes\n",
    "    'batch_sizes': [256, 1024, 16384],\n",
    "    \n",
    "    # Epochs\n",
    "    'epochs': [100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20bb31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the grid search function\n",
    "def grid_search(x_train, y_train, configs, k=5):\n",
    "    \"\"\"\n",
    "    Perform grid search on the space of possible parameters defined earlier using k-fold cross validation\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate all combinations\n",
    "    combinations = list(itertools.product(\n",
    "        configs['architectures'],\n",
    "        configs['activations'],\n",
    "        configs['learning_rates'],\n",
    "        configs['batch_sizes'],\n",
    "        configs['epochs']\n",
    "    ))\n",
    "\n",
    "    total_combinations = len(combinations)\n",
    "    print(f\"Total configurations to test: {total_combinations}\")\n",
    "    print(f\"With k={k} folds, total training runs: {total_combinations * k}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    best_acc = -1\n",
    "    best_loss = -1\n",
    "    best_config = None\n",
    "    best_learning_curves = None\n",
    "    best_fold_results = None\n",
    "\n",
    "    for idx, (layer_widths, activation, lr, batch_size, epochs) in enumerate(combinations):\n",
    "        print(f\"\\n[{idx+1}/{total_combinations}] Testing configuration:\")\n",
    "        print(f\"Architecture: {layer_widths}\")\n",
    "        print(f\"Activation: {activation}\")\n",
    "        print(f\"Learning Rate: {lr}\")\n",
    "        print(f\"Batch Size: {batch_size}\")\n",
    "        print(f\"Epochs: {epochs}\")\n",
    "        \n",
    "        mean_val_acc, mean_val_loss, fold_results, learning_curves = train_model_kfold(\n",
    "            x_train, y_train, layer_widths, activation, lr, batch_size, epochs, k=k\n",
    "        )\n",
    "        \n",
    "        # result\n",
    "        config = {\n",
    "            'layer_widths': layer_widths,\n",
    "            'activation': activation,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size,\n",
    "            'epochs': epochs\n",
    "        }\n",
    "        \n",
    "        print(f\"Mean Validation Accuracy: {mean_val_acc:.4f} ({mean_val_acc*100:.2f}%)\")\n",
    "        print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "        \n",
    "        # Update best configuration\n",
    "        if mean_val_acc > best_acc:\n",
    "            best_acc = mean_val_acc\n",
    "            best_loss = mean_val_loss\n",
    "            best_config = config.copy()\n",
    "            best_learning_curves = learning_curves\n",
    "            best_fold_results = fold_results\n",
    "            print(f\"New best configuration!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Grid Search Complete!\")\n",
    "    print(f\"Best Configuration:\")\n",
    "    print(f\"Architecture: {best_config['layer_widths']}\")\n",
    "    print(f\"Activation: {best_config['activation']}\")\n",
    "    print(f\"Learning Rate: {best_config['learning_rate']}\")\n",
    "    print(f\"Batch Size: {best_config['batch_size']}\")\n",
    "    print(f\"Epochs: {best_config['epochs']}\")\n",
    "    print(f\"Best Mean Validation Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return best_config, best_acc, best_loss, best_learning_curves, best_fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations to test: 36\n",
      "With k=5 folds, total training runs: 180\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1/36] Testing configuration:\n",
      "Architecture: [784, 128, 128, 10]\n",
      "Activation: relu\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 256\n",
      "Epochs: 100\n",
      "Fold 1, Epoch 0: Val Acc = 0.2330, Val Loss = 2.1852\n"
     ]
    }
   ],
   "source": [
    "# Run this if you want to search and train, but hopefully some params are saved :)\n",
    "x = mnist.data.to_numpy()\n",
    "y = mnist.target.astype(int).to_numpy()\n",
    "x_train, y_train, x_test, y_test = get_splits(x, y)\n",
    "\n",
    "# Run grid search to find best configs\n",
    "best_config, best_acc, best_loss, best_learning_curves, best_fold_results = grid_search(\n",
    "    x_train, y_train, \n",
    "    grid_search_configs, \n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Print out the best accuracy and learning curve for kfold training\n",
    "print(\"The best configuration is:\")\n",
    "print(best_config)\n",
    "print(f\"Best K-fold Mean Accuracy: {best_acc:.2f} ({best_acc*100:.2f}%)\")\n",
    "print(f\"Best K-fold Mean Loss: {best_loss:.2f}\")\n",
    "\n",
    "# Train best model on these params\n",
    "params, test_acc, test_loss, learning_curve = train_model(\n",
    "    x_train, y_train, x_test, y_test, best_config\n",
    ")\n",
    "\n",
    "# Print out the best accuracy and learning curve\n",
    "print(f\"Our best model trained on the full training set has an accuracy of {test_acc}%\")\n",
    "print(f\"Our best model trained on the full training set has a loss of {test_loss:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(learning_curve['train_acc'], label='Training Accuracy')\n",
    "plt.plot(learning_curve['test_acc'], label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(learning_curve['train_loss'], label='Training Loss')\n",
    "plt.plot(learning_curve['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "save_params(params, filename='assets/mnist_params.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ead2d0",
   "metadata": {},
   "source": [
    "   (b) Study how optimizer hyperparameters (batch size, learning rate) affect convergence speed and final performance, and discuss your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94dd50e",
   "metadata": {},
   "source": [
    " (c) Identify and visualize misclassified images for your best model, and provide possible explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
