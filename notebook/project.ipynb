{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4668a816-21f1-4253-b3eb-2d0b16cee877",
   "metadata": {},
   "source": [
    "# Linear Algebra and Optimization for Machine Learning - Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a401d06b-9aea-4898-aa47-cbffd3118c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85926fc1-fb59-498a-8449-660f0a868ef9",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "(a) Generate a $300 \\times 20$ data matrix $X$, where each entry is uniformly random.  \n",
    "Generate an outcome vector $y$, which is a linear combination of the columns of $X$ with uniformly random weights, and some Gaussian noise added to each entry of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c644e6b7-7ae7-4e2f-9a62-b955054e7f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is:  (300, 20)\n"
     ]
    }
   ],
   "source": [
    "# shape\n",
    "m = 300\n",
    "n = 20\n",
    "\n",
    "# feature matrix, Xij ~ Unif([0, 1]), X.shape = (300, 20)\n",
    "X = np.random.uniform(size = (m, n))\n",
    "\n",
    "# generate random noise eps, eps.shape = (300,)\n",
    "eps = np.random.normal(size = m)\n",
    "\n",
    "# uniformly random weights weight, weight.shape = (20,)\n",
    "weight = np.random.uniform(size = n)\n",
    "\n",
    "# y = X weight + eps, y.shape = (300,)\n",
    "y = X @ weight + eps\n",
    "\n",
    "# print shape of the data\n",
    "print(\"The shape of the data is: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261f9a1-dae5-4659-8822-06a9dfb431cc",
   "metadata": {},
   "source": [
    "(b) Write a function to divide the data set into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a266dbef-d000-4010-b25f-ea17a180ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for splitting into training and testing datasets and option for validation set\n",
    "def train_test_split(X, y, p_train: float, p_val: float = None):\n",
    "    '''\n",
    "    splits feature matrix and response vector according to given split percentage\n",
    "    either splits into training and testing set (if p_val = None)\n",
    "    or training, validation, and testing set (if p_val != None)\n",
    "    \n",
    "    (X, y): datapoints\n",
    "    p_train: fraction of data that is in training set --> round up\n",
    "    p_val: fraction of data that is in validation set --> round up\n",
    "    \n",
    "    returns: X_train, (X_val), X_test, y_train, (y_val), y_test\n",
    "    '''\n",
    "\n",
    "    # shape of feature matrix\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    # calculate split index for training set\n",
    "    split_train = int(np.ceil(p_train * m))\n",
    "\n",
    "    # split feature and response according to split index\n",
    "    # check if validation set is wanted\n",
    "    if p_val is None:\n",
    "        # just training and testing\n",
    "        X_train, X_test = X[:split_train], X[split_train:]\n",
    "        y_train, y_test = y[:split_train], y[split_train:]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        # calculate validation split index\n",
    "        split_val = int(np.ceil((split_train + p_val * m)))\n",
    "        X_train, X_val, X_test = X[:split_train], X[split_train:split_val], X[split_val:]\n",
    "        y_train, y_val, y_test = y[:split_train], y[split_train:split_val], y[split_val:]\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74a176c6-58cc-452e-9a35-0aca10a0e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage\n",
    "p = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, p)\n",
    "\n",
    "# with validation set\n",
    "p_train = 0.7\n",
    "p_val = 0.15\n",
    "\n",
    "X_train_v, X_val_v, X_test_v, y_train_v, y_val_v, y_test_v = train_test_split(X, y, p_train, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941921f-5410-432e-b730-7eff48027a3e",
   "metadata": {},
   "source": [
    "(c) Write functions for OLS and Ridge regression and apply this to your synthetic data set. Discuss the performance on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "526d5d95-7579-4243-a181-03364133e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MSE of prediction\n",
    "def evaluate(X, y, w):\n",
    "    '''\n",
    "    evaluates the performance (MSE) of some weights matrix w for features X and response y\n",
    "    assumes linear relationship --> Xw = y\n",
    "    \n",
    "    (X, y): datapoints\n",
    "    w: weights vector\n",
    "\n",
    "    returns: MSE\n",
    "    '''\n",
    "    MSE = np.mean((X @ w - y)**2)\n",
    "    return MSE\n",
    "\n",
    "# OLS solution using normal equations\n",
    "# use Moore-Penrose inverse --> if the inverse exists, then X(-1) == X(+)\n",
    "def OLS(X_train, y_train):\n",
    "    '''\n",
    "    gives least-norm OLS solution for the given feature matrix and corresponding response vector\n",
    "    uses the moore-penrose pseudo-inverse where w = X+ y\n",
    "    when the inverse exists, X+ = X-1\n",
    "\n",
    "    (X_train, y_train): datapoints\n",
    "\n",
    "    returns: weights vector w\n",
    "    '''\n",
    "    # solve using the pseudo-inverse (least-norm OLS solution)\n",
    "    w = np.linalg.pinv(X_train) @ y_train\n",
    "\n",
    "    return w\n",
    "    \n",
    "\n",
    "# Ridge Regression\n",
    "def ridge(X_train, y_train, lamb):\n",
    "    '''\n",
    "    gives solution to ridge regression for the given feature matrix and corresponding response vector with regularization term lamb\n",
    "\n",
    "    (X_train, y_train): datapoints\n",
    "    lamb: regularization parameter\n",
    "\n",
    "    returns: weights vector w\n",
    "    '''\n",
    "    # number of datapoints\n",
    "    n = X_train.shape[1]\n",
    "\n",
    "    # unique minimizer for Ridge Regression\n",
    "    w = np.linalg.inv(X_train.T @ X_train + lamb * np.eye(n)) @ X_train.T @ y_train\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "# find optimal lambda using validation set\n",
    "def find_opt_lam(X_train, y_train, X_val, y_val):\n",
    "    '''\n",
    "    finds optimal value for lambda for ridge regression\n",
    "    uses training set to find w for a given lambda\n",
    "    uses validation set to compare performance of w\n",
    "    \n",
    "    (X_train, y_train): datapoints in training set\n",
    "    (X_val, y_val): datapoints in validation set\n",
    "\n",
    "    returns: opt_lam\n",
    "    '''\n",
    "    # initialize performance and optimal lambda values\n",
    "    min_perf = float('inf')\n",
    "    opt_lam = None\n",
    "\n",
    "    # determine performance on validation set and update opt_lam if performance is better\n",
    "    for lamb in np.arange(0.01, 100, 0.01):\n",
    "        w_ridge = ridge(X_train, y_train, lamb)\n",
    "        perf_val = evaluate(X_val, y_val, w_ridge)\n",
    "\n",
    "        if perf_val < min_perf:\n",
    "            min_perf = perf_val\n",
    "            opt_lam = lamb\n",
    "\n",
    "    return opt_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7684337-309f-4b3f-acfb-b00276989c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 0.8847\n",
      "OLS test MSE: 1.5692\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.9029\n",
      "ridge test MSE: 1.5474\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "# use training, validation, test split to keep comparisons fair\n",
    "\n",
    "# OLS solution\n",
    "w_OLS = OLS(X_train_v, y_train_v)\n",
    "\n",
    "# Ridge solution\n",
    "# find optimal lambda using training and validation set\n",
    "opt_lam = find_opt_lam(X_train_v, y_train_v, X_test_v, y_test_v)\n",
    "\n",
    "# solve ridge\n",
    "w_ridge = ridge(X_train_v, y_train_v, opt_lam)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = evaluate(X_train_v, y_train_v, w_OLS), evaluate(X_test_v, y_test_v, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = evaluate(X_train_v, y_train_v, w_ridge), evaluate(X_test_v, y_test_v, w_ridge)\n",
    "\n",
    "# print performance\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e37501",
   "metadata": {},
   "source": [
    "### Question 1 (c) — Performance Discussion\n",
    "\n",
    "We compare **Ordinary Least Squares (OLS)** and **Ridge Regression** on the synthetic dataset.\n",
    "\n",
    "| Model | Train MSE | Test MSE | Observation |\n",
    "|:--|:--:|:--:|:--|\n",
    "| **OLS** | 0.8847 | 1.5692 | Low train error but higher test error → slight overfitting |\n",
    "| **Ridge** | 0.9029 | 1.5474 | Slightly higher train error, lower test error → better generalization |\n",
    "\n",
    "The OLS model achieves the lowest error on the training set, showing it fits the data well.  \n",
    "\n",
    "However, its test MSE is higher, meaning it captures some noise from the training data and thus overfits slightly.  \n",
    "\n",
    "Ridge regression introduces an ℓ₂-penalty that shrinks the coefficients, trading a bit of bias for reduced variance.  \n",
    "\n",
    "As expected, the training MSE increases a little (0.8847 → 0.9029), but the test MSE decreases (1.5692 → 1.5474).  \n",
    "\n",
    "This demonstrates the bias–variance trade-off: by constraining the model complexity, Ridge achieves better generalization on unseen data.  \n",
    "\n",
    "The improvement is modest, which suggests the dataset is not strongly affected by multicollinearity or noise, yet regularization still provides a small stability gain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37762593-cde2-4745-a57a-a61264c677c3",
   "metadata": {},
   "source": [
    "(d) Create a data matrix with many multicolinearities by adding a large number (say, 200) columns to X that are linear combinations of the original 20 columns with some Gaussian noise added to each entry. Run OLS and Ridge regression and discuss the performance on train and test sets. Is it hard to find a good value for $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4caf3c3c-2c05-4d71-b0ad-f3353e00eb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_multicolinearity(X, num):\n",
    "    '''\n",
    "    add multicolinearity to feature matrix X by adding (num) columns that are linear combinations of the original columns\n",
    "    weights for linear combinations are uniformly random\n",
    "    gaussian noise added to each entry\n",
    "\n",
    "    X: feature matrix\n",
    "    num: number of new columns to be added\n",
    "\n",
    "    returns: X_new\n",
    "    '''\n",
    "    # number of entries, features\n",
    "    n_entries = X.shape[0]\n",
    "    n_feats = X.shape[1]\n",
    "\n",
    "    # initialize new columns list\n",
    "    new_cols = []\n",
    "    \n",
    "    for i in range(num):\n",
    "        # uniformly random weights w, w.shape = (n_feats, )\n",
    "        w = np.random.uniform(size = n_feats)\n",
    "\n",
    "        # generate random noise eps, eps.shape = (n_entries,)\n",
    "        eps = np.random.normal(size = n_entries)\n",
    "\n",
    "        # linear combination of features + noise\n",
    "        X_col = X @ w + eps\n",
    "        new_cols.append(X_col.reshape(-1, 1))\n",
    "\n",
    "    # concatenate the X with new columns\n",
    "    X_new = np.hstack([X] + new_cols)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "648e26cb-70b8-4d32-9c39-f7a089d9335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The multicolinearity data shape is:  (300, 220)\n",
      "\n",
      "The optimal lambda is:  99.99\n",
      "\n",
      "OLS train MSE: 0.0\n",
      "OLS test MSE: 21.9828\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.2979\n",
      "ridge test MSE: 1.7687\n"
     ]
    }
   ],
   "source": [
    "# add multicolinearity\n",
    "X_new = add_multicolinearity(X, 200)\n",
    "print(\"The multicolinearity data shape is: \", X_new.shape)\n",
    "\n",
    "# split with validation set\n",
    "p_train = 0.7\n",
    "p_val = 0.15\n",
    "X_train_new, X_val_new, X_test_new, y_train_new, y_val_new, y_test_new = train_test_split(X_new, y, p_train, p_val)\n",
    "\n",
    "# find optimal lambda\n",
    "opt_lam = find_opt_lam(X_train_new, y_train_new, X_val_new, y_val_new)\n",
    "print(\"\\nThe optimal lambda is: \", np.round(opt_lam, 4))\n",
    "\n",
    "# example usage\n",
    "w_OLS = OLS(X_train_new, y_train_new)\n",
    "w_ridge = ridge(X_train_new, y_train_new, opt_lam)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = evaluate(X_train_new, y_train_new, w_OLS), evaluate(X_test_new, y_test_new, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = evaluate(X_train_new, y_train_new, w_ridge), evaluate(X_test_new, y_test_new, w_ridge)\n",
    "\n",
    "# print performace\n",
    "print(f\"\\nOLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43139575",
   "metadata": {},
   "source": [
    "### Question 1 (d) — Performance Discussion\n",
    "\n",
    "After adding 200 multicollinear columns, the dataset became highly redundant. The OLS model has a train MSE of 0.0 and a test MSE of 21.9828. The test error is an order of magnitude higher, which shows that the model is affected by the multicollinearity.\n",
    "\n",
    "In contrast, ridge regression performed much better, with a train MSE of 0.2979 and a test MSE of 1.7687. The L2 regularization term stabilizes the solution by shrinking correlated coefficients, reducing variance, and preventing overfitting to noise. The chosen lambda value of approximately 100 shows that strong regularization was required to counteract the effects of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b439ffc-fff8-472f-a0a9-c92f6c095e11",
   "metadata": {},
   "source": [
    "(e) Now instead of adding multicolinearities, add many superficial feature columns to X which have no relation to the outcome vector y. Again run OLS and Ridge regression and discuss the performance on train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61d6325a-a5c4-48e0-902f-366ec6bffad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_superficial(X, num):\n",
    "    '''\n",
    "    adds (num) superficial features to feature matrix X --> have no relation to response vector\n",
    "\n",
    "    X: feature matrix\n",
    "    num: number of superficial features to add\n",
    "\n",
    "    returns: X_new\n",
    "    '''\n",
    "    # number of entries\n",
    "    n_entries = X.shape[0]\n",
    "\n",
    "    # uniformly random features\n",
    "    sup_feats = np.random.uniform(size = (n_entries, num))\n",
    "\n",
    "    # concatenate the X with new columns\n",
    "    X_new = np.hstack([X, sup_feats])\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f82666f-fc34-45d7-90ef-d3b8bd89458c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The superficial data shape is:  (300, 220)\n",
      "\n",
      "The optimal lambda is:  43.07\n",
      "\n",
      "OLS train MSE: 0.0\n",
      "OLS test MSE: 30.2725\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.7491\n",
      "ridge test MSE: 2.1213\n"
     ]
    }
   ],
   "source": [
    "# add superficial features\n",
    "X_new = add_superficial(X, 200)\n",
    "print(\"The superficial data shape is: \", X_new.shape)\n",
    "\n",
    "# split with validation set\n",
    "p_train = 0.7\n",
    "p_val = 0.15\n",
    "X_train_new, X_val_new, X_test_new, y_train_new, y_val_new, y_test_new = train_test_split(X_new, y, p_train, p_val)\n",
    "\n",
    "# find optimal lambda\n",
    "opt_lam = find_opt_lam(X_train_new, y_train_new, X_val_new, y_val_new)\n",
    "print(\"\\nThe optimal lambda is: \", round(opt_lam, 4))\n",
    "\n",
    "# example usage\n",
    "w_OLS = OLS(X_train_new, y_train_new)\n",
    "w_ridge = ridge(X_train_new, y_train_new, opt_lam)\n",
    "\n",
    "# performance\n",
    "perf_OLS_tr, perf_OLS_te = evaluate(X_train_new, y_train_new, w_OLS), evaluate(X_test_new, y_test_new, w_OLS)\n",
    "perf_ridge_tr, perf_ridge_te = evaluate(X_train_new, y_train_new, w_ridge), evaluate(X_test_new, y_test_new, w_ridge)\n",
    "\n",
    "print(f\"\\nOLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834959f5",
   "metadata": {},
   "source": [
    "### Question 1 (e) — Performance Discussion\n",
    "\n",
    "When adding many superficial features unrelated to the outcome, we get a similar effect as before. The OLS model again has a train MSE of 0.0 and a test MSE of 30.2725. The order of magnitude between train and test error indicates that OLS overfits the noise introduced by the irrelevant features, resulting in poor generalization.\n",
    "\n",
    "Ridge regression handled the situation much better, with a train MSE of 0.7491 and a test MSE of 2.1213. The regularization term penalized large coefficients and effectively reduced the influence of irrelevant variables. The optimal lambda of around 13.03 suggests that moderate regularization was sufficient to control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358f6da-dfea-46ec-aadf-1590abadb4d8",
   "metadata": {},
   "source": [
    "## 2.\n",
    "\n",
    "(a) Implement functions for logistic regression and hinge-loss classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a094b7b2-d582-4cec-8ec9-12fc48eb6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new evals function since they have a different model\n",
    "def evaluate_logistic(X, y, w):\n",
    "    '''\n",
    "    evaluates the performance of logistic regression and returns the mean squared error\n",
    "    '''\n",
    "    y_pred = logistic_predict(X, w)\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    return mse\n",
    "\n",
    "def evaluate_hinge(X, y, w):\n",
    "    '''\n",
    "    evaluates the performance of hinge-loss and returns the mean squared error\n",
    "    '''\n",
    "    y_pred = hinge_predict(X, w)\n",
    "    mse = np.mean((y - y_pred)**2)\n",
    "    return mse\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def logistic_predict(X, w, threshold=0.5):\n",
    "    p = sigmoid(X @ w) # this is the probability of the data point being in class 1 (so between 0 and 1)\n",
    "    return np.where(p >= threshold, 1, -1)\n",
    "\n",
    "def hinge_predict(X, w):\n",
    "    return np.where(X @ w >= 0, 1, -1)\n",
    "\n",
    "def train_logistic_gd(X, y, lam, lr=0.01, num_iter=1000):\n",
    "    '''\n",
    "    train the logistic regression model and returns the weight vector\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        p = sigmoid(-y * (X @ w))\n",
    "        grad_w = -(X.T @ (p * y)) / n + lam * w\n",
    "        w -= lr * grad_w\n",
    "    return w\n",
    "\n",
    "def train_hinge_gd(X, y, lam, C=1.0, lr=0.01, num_iter=1000):\n",
    "    '''\n",
    "    train the hinge-loss model and returns the weight vector\n",
    "    '''\n",
    "    n, d = X.shape\n",
    "    w = np.zeros(d)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        scores = X @ w\n",
    "        viol = y * scores < 1\n",
    "\n",
    "        grad_w = w - C * (X[viol].T @ y[viol]) + lam * w\n",
    "\n",
    "        # update\n",
    "        w -= lr * grad_w\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280739c-a503-4770-8bac-b7a8d4ada41c",
   "metadata": {},
   "source": [
    "(b) Create a random data matrix $X$ and construct an output vector $y$ by generating and a random weight vector $w$ and setting $y_i = \\text{sign}(x^T_i w)$, where $x^T_i$ is the $i$-th row of $X$. Use a test/train split and check the performance of OLS, Ridge regression, logistic regression and hinge-loss classification for binary classification. Do you see a large difference in performance between these methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ac471b7-5f98-41e1-95c9-184195ce29c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 0.3514\n",
      "OLS test MSE: 0.3624\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.3514\n",
      "ridge test MSE: 0.3624\n",
      "-----------------------------------\n",
      "logistic train MSE: 0.0952\n",
      "logistic test MSE: 0.3111\n",
      "-----------------------------------\n",
      "hinge train MSE: 0.0381\n",
      "hinge test MSE: 0.0889\n"
     ]
    }
   ],
   "source": [
    "# create a random data matrix X\n",
    "n = 300\n",
    "d = 20\n",
    "\n",
    "X = np.random.normal(size = (n, d))\n",
    "w = np.random.normal(size = d)\n",
    "y = np.sign(X @ w)\n",
    "\n",
    "p = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, p)\n",
    "\n",
    "# Set lambda\n",
    "lam = 0.01\n",
    "\n",
    "# perform OLS\n",
    "w_OLS = OLS(X_train, y_train)\n",
    "perf_OLS_tr, perf_OLS_te = evaluate(X_train, y_train, w_OLS), evaluate(X_test, y_test, w_OLS)\n",
    "\n",
    "# perform Ridge\n",
    "w_ridge = ridge(X_train, y_train, lam)\n",
    "perf_ridge_tr, perf_ridge_te = evaluate(X_train, y_train, w_ridge), evaluate(X_test, y_test, w_ridge)\n",
    "\n",
    "# perform logistic regression\n",
    "w_log = train_logistic_gd(X_train, y_train, lam)\n",
    "perf_log_tr, perf_log_te = evaluate_logistic(X_train, y_train, w_log), evaluate_logistic(X_test, y_test, w_log)\n",
    "\n",
    "# perform hinge loss\n",
    "w_hinge = train_hinge_gd(X_train, y_train, lam)\n",
    "perf_hinge_tr, perf_hinge_te = evaluate_hinge(X_train, y_train, w_hinge), evaluate_hinge(X_test, y_test, w_hinge)\n",
    "\n",
    "# print all performances\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"logistic train MSE: {np.round(perf_log_tr, 4)}\")\n",
    "print(f\"logistic test MSE: {np.round(perf_log_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"hinge train MSE: {np.round(perf_hinge_tr, 4)}\")\n",
    "print(f\"hinge test MSE: {np.round(perf_hinge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae34c2-f82f-4e22-a0d8-08e14ed9d4a5",
   "metadata": {},
   "source": [
    "(c) Now create a data set $(X, y)$ for binary classification (with $X \\in \\mathbb{R}^{n \\times d}$ and $y \\in \\{−1, 1\\}^n$) such that, given a test/train split, OLS and Ridge perform very badly but logistic regression and hinge-loss classification perform well. What kind of properties of your data set are responsible for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62a7f2c5-ee48-444c-a331-cfd4857b3982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS train MSE: 0.687\n",
      "OLS test MSE: 0.8228\n",
      "-----------------------------------\n",
      "ridge train MSE: 0.687\n",
      "ridge test MSE: 0.8228\n",
      "-----------------------------------\n",
      "logistic train MSE: 0.0\n",
      "logistic test MSE: 0.0\n",
      "-----------------------------------\n",
      "hinge train MSE: 0.0\n",
      "hinge test MSE: 0.0\n"
     ]
    }
   ],
   "source": [
    "# create a random data matrix X\n",
    "n = 300\n",
    "d = 20\n",
    "\n",
    "X1 = np.random.normal(loc=1, scale=1, size = (250, d))\n",
    "X2 = np.random.normal(loc=100, scale=0.1, size =(50, d))\n",
    "X = np.concatenate([X1, X2])\n",
    "np.random.shuffle(X)\n",
    "w = np.random.uniform(low=-0.05, high=0.95, size = d) # Uniform distribution which is heavily skewed to being positive\n",
    "y = np.sign(X @ w)\n",
    "\n",
    "p = 0.7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, p)\n",
    "\n",
    "# Set lambda\n",
    "lam = 0.01\n",
    "\n",
    "# perform OLS\n",
    "w_OLS = OLS(X_train, y_train)\n",
    "perf_OLS_tr, perf_OLS_te = evaluate(X_train, y_train, w_OLS), evaluate(X_test, y_test, w_OLS)\n",
    "\n",
    "# perform Ridge\n",
    "w_ridge = ridge(X_train, y_train, lam)\n",
    "perf_ridge_tr, perf_ridge_te = evaluate(X_train, y_train, w_ridge), evaluate(X_test, y_test, w_ridge)\n",
    "\n",
    "# perform logistic regression\n",
    "w_log = train_logistic_gd(X_train, y_train, lam)\n",
    "perf_log_tr, perf_log_te = evaluate_logistic(X_train, y_train, w_log), evaluate_logistic(X_test, y_test, w_log)\n",
    "\n",
    "# perform hinge loss\n",
    "w_hinge = train_hinge_gd(X_train, y_train, lam)\n",
    "perf_hinge_tr, perf_hinge_te = evaluate_hinge(X_train, y_train, w_hinge), evaluate_hinge(X_test, y_test, w_hinge)\n",
    "\n",
    "# print all performances\n",
    "print(f\"OLS train MSE: {np.round(perf_OLS_tr, 4)}\")\n",
    "print(f\"OLS test MSE: {np.round(perf_OLS_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"ridge train MSE: {np.round(perf_ridge_tr, 4)}\")\n",
    "print(f\"ridge test MSE: {np.round(perf_ridge_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"logistic train MSE: {np.round(perf_log_tr, 4)}\")\n",
    "print(f\"logistic test MSE: {np.round(perf_log_te, 4)}\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"hinge train MSE: {np.round(perf_hinge_tr, 4)}\")\n",
    "print(f\"hinge test MSE: {np.round(perf_hinge_te, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57c148",
   "metadata": {},
   "source": [
    "### Question 2 (c) — Discussion\n",
    "The data we generated has unbalanced classes, this is a result of the w vector and each x_i being chosen from a distribution which is heavily skewed to being positive. Thus, when we generate the y vector with the same formula used in 2 (b) most of the data points will be classified in the same class. The data also has many points which very clearly belong to a particular class, this comes from the 'X2' points which are narrowly distributed very positive points when compared to the overall distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
