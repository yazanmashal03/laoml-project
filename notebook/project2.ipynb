{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f4da8162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import JAX to use\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "55c14307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a dense feedforward neural network from scratch. Make your implementation flexible with\n",
    "# respect to the input and output dimensions, the number of hidden layers and neurons per hidden\n",
    "# layer, and the activation functions used. Use this implementation for the following two questions.\n",
    "# Choose a suitable initialization for the network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with a function that initializes the network parameters.\n",
    "def init_net_params(layer_widths, key):\n",
    "    \"\"\"\n",
    "    Initialize the network parameters.\n",
    "    layer_widths: list of integers, the number of neurons in each layer\n",
    "    key: jax.random.PRNGKey, the random key\n",
    "    returns: list of jax.numpy.ndarray, the network parameters\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_widths) - 1)\n",
    "\n",
    "    for i, (n_in, n_out) in enumerate(zip(layer_widths[:-1], layer_widths[1:])):\n",
    "        w_key = keys[i]\n",
    "        w = random.normal(w_key, shape=(n_in, n_out))\n",
    "        b = jnp.zeros((n_out,))\n",
    "        params.append({'w': w, 'b': b})\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "27ef9371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define a forward pass function that computes the output of the network for a given input.\n",
    "def forward(params, x, activation):\n",
    "    \"\"\"\n",
    "    Forward pass of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    # dictionary of activation functions\n",
    "    activations = {\n",
    "        'relu': jax.nn.relu,\n",
    "        'sigmoid': jax.nn.sigmoid,\n",
    "        'tanh': jax.nn.tanh,\n",
    "        'softmax': jax.nn.softmax\n",
    "    }\n",
    "    activation = activations[activation]\n",
    "\n",
    "    for layer in params[:-1]:\n",
    "        x = x @ layer['w'] + layer['b']\n",
    "        x = activation(x)\n",
    "\n",
    "    # output layer\n",
    "    final_layer = params[-1]\n",
    "    return jnp.dot(x, final_layer['w']) + final_layer['b']\n",
    "\n",
    "def get_batches(x, y, batch_size=256):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples (x_batch, y_batch), each of size batch_size\n",
    "    (last batch may be smaller).\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    \n",
    "    # Use provided key or create a new one\n",
    "    key = random.PRNGKey(0)\n",
    "    perm = jax.random.permutation(key, n)\n",
    "    x_shuffled = x[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    batches = []\n",
    "    for i in range(0, n, batch_size):\n",
    "        x_batch = x_shuffled[i:i+batch_size]\n",
    "        y_batch = y_shuffled[i:i+batch_size]\n",
    "        batches.append((x_batch, y_batch))\n",
    "\n",
    "    return batches\n",
    "\n",
    "def get_splits(x, y, train=0.8, valid=0.1):\n",
    "    \"\"\"\n",
    "    This return a jax array of the training, validation, and test splits\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    x = jnp.array(x)\n",
    "    y = jnp.array(y)\n",
    "    # Calculate split indices (as integers)\n",
    "    train_end = int(train * n)\n",
    "    valid_end = train_end + int(valid * n)\n",
    "    \n",
    "    # Split the data\n",
    "    x_train = x[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    \n",
    "    x_valid = x[train_end:valid_end]\n",
    "    y_valid = y[train_end:valid_end]\n",
    "    \n",
    "    x_test = x[valid_end:]\n",
    "    y_test = y[valid_end:]\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "c460bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the MSE loss function, we can have other loss functions\n",
    "def mse_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    MSE loss function for the network.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    preds = batched_forward(params, x, activation)\n",
    "    return jnp.mean((preds - y) ** 2)\n",
    "\n",
    "def class_loss(params, x, y, activation):\n",
    "    \"\"\"\n",
    "    Classification cross-entroy loss function\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "    log_probs = jax.nn.softmax(logits, axis=1)\n",
    "    \n",
    "    nll = -log_probs[jnp.arange(y.shape[0]), y]\n",
    "    loss = jnp.mean(nll)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(params, x, y, activation, classification):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataset and return accuracy and loss.\n",
    "    \"\"\"\n",
    "    batched_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "    logits = batched_forward(params, x, activation)\n",
    "\n",
    "    if classification:\n",
    "        preds = jnp.argmax(logits, axis=1)\n",
    "        \n",
    "        accuracy = jnp.mean(preds == y)\n",
    "        loss = class_loss(params, x, y, activation)\n",
    "        \n",
    "        return accuracy, loss\n",
    "    else:\n",
    "        return jnp.mean((logits - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "d3f55d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now define an update function that updates the network parameters.\n",
    "def update(params, x, y, activation, lr, classification):\n",
    "    \"\"\"\n",
    "    Update function for the network parameters (basic gradient descent).\n",
    "    \"\"\"\n",
    "    loss_fn = class_loss if classification else mse_loss\n",
    "    grads = grad(loss_fn)(params, x, y, activation)\n",
    "    new_params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9a855b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 70000 images\n",
      "Each image has 784 pixels (features)\n"
     ]
    }
   ],
   "source": [
    "# Consider a standard benchmark dataset for classification: train a neural network to classify handwrit-\n",
    "# ten digits into the ten classes 0, 1, . . . , 9. As input for your model, use flattened vector representations\n",
    "# of the MNIST images\n",
    "mnist = fetch_openml('mnist_784')\n",
    "print(f\"We have {mnist.data.shape[0]} images\")\n",
    "print(f\"Each image has {mnist.data.shape[1]} pixels (features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0, Train Loss: -0.1059, Val Loss: -0.0936\n",
      "Epoch 100, Train Loss: -0.1059, Val Loss: -0.0936\n"
     ]
    }
   ],
   "source": [
    "# Training the neural network\n",
    "key = random.PRNGKey(42)\n",
    "layer_widths = [784, 256, 256, 10]\n",
    "epochs = 500\n",
    "params = init_net_params(layer_widths, key)\n",
    "x = mnist.data.to_numpy()\n",
    "y = mnist.target.astype(int).to_numpy()\n",
    "x_train, y_train, x_valid, y_valid, x_test, y_test = get_splits(x, y)\n",
    "activation = 'relu'\n",
    "batch_size = 128\n",
    "lr = 0.01\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # Get batches for this epoch (shuffles data each epoch)\n",
    "    batches = get_batches(x_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    for x_batch, y_batch in batches:\n",
    "        params = update(params, x_batch, y_batch, activation, lr, classification=True) \n",
    "    # Print progress every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        train_loss = class_loss(params, x_train, y_train, activation)\n",
    "        val_loss = class_loss(params, x_valid, y_valid, activation)\n",
    "        print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a731c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 33.03%\n",
      "Test Loss: -0.33\n",
      "Validation Accuracy: 31.93%\n",
      "Validation Loss: -0.32\n"
     ]
    }
   ],
   "source": [
    "# We now test the model\n",
    "# Test on test set\n",
    "test_acc, test_loss = evaluate_model(params, x_test, y_test, activation, classification=True)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "# Also check validation set\n",
    "val_acc, val_loss = evaluate_model(params, x_valid, y_valid, activation, classification=True)\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "print(f\"Validation Loss: {val_loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
